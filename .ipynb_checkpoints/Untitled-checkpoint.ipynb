{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1e68ef6f-3d2f-41c7-8d79-ad98cdcc89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = (\n",
    "    pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/m-clark/generalized-additive-models/master/data/pisasci2006.csv\"\n",
    "    )\n",
    "    .drop(columns=\"Country\")\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "class BaseMetrics:\n",
    "    def __init__(self, X, y, intercept):\n",
    "        if intercept:\n",
    "            self.X = np.insert(X, 0, 1, axis=1)\n",
    "            self._cols = X.columns.insert(0, \"Intercept\")\n",
    "        else:\n",
    "            self.X = X.to_numpy()\n",
    "            self._cols = X.columns\n",
    "\n",
    "        self.intercept = intercept\n",
    "        self._n = self.X.shape\n",
    "        self._ycol = y.name\n",
    "        self.y = y.to_numpy()\n",
    "        self.comp = np.linalg.inv(self.X.T @ self.X)\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.comp @ self.X.T @ self.y\n",
    "\n",
    "    def fitted(self):\n",
    "        return self.X @ self.coefficients()\n",
    "\n",
    "    def residuals(self):\n",
    "        return self.y - self.fitted()\n",
    "\n",
    "    def _ss_error(self):\n",
    "        return (self.residuals() ** 2).sum()\n",
    "\n",
    "    def _ss_total(self):\n",
    "        return (self.y**2).sum() - (\n",
    "            self.y.sum() ** 2 / self._n[0] if self.intercept else 0\n",
    "        )\n",
    "\n",
    "    def _ss_model(self):\n",
    "        return self._ss_total() - self._ss_error()\n",
    "\n",
    "    def df_error(self):\n",
    "        return self._n[0] - self._n[1]\n",
    "\n",
    "    def sigma(self):\n",
    "        return np.sqrt(self._ss_error() / self.df_error())\n",
    "\n",
    "    def _mse_total(self):\n",
    "        return self._ss_total() / (self._n[0] - 1)\n",
    "\n",
    "    def df_model(self):\n",
    "        return self._n[1] - 1 if self.intercept else self._n[1]\n",
    "\n",
    "    def _mse_model(self):\n",
    "        return self._ss_model() / self.df_model()\n",
    "\n",
    "    def f_value(self):\n",
    "        return self._mse_model() / (self.sigma() ** 2)\n",
    "\n",
    "    def _loglik(self):\n",
    "\n",
    "        return (\n",
    "            -self._n[0] * np.log(2 * np.pi) / 2\n",
    "            - self._n[0] * np.log(self.sigma())\n",
    "            - self._ss_error() / (2 * self.sigma() ** 2)\n",
    "        )\n",
    "\n",
    "    def _aic(self):\n",
    "        return -2 * self._loglik() + 2 * self._n[1]\n",
    "\n",
    "    def _bic(self):\n",
    "        return -2 * self._loglik() + self._n[1] * np.log(self._n[0])\n",
    "\n",
    "    def f_pvalue(self):\n",
    "        return 1 - st.f.cdf(self.f_value(), self.df_model(), self.df_error())\n",
    "\n",
    "    def rsquared(self):\n",
    "        return self._ss_model() / self._ss_total()\n",
    "\n",
    "    def adj_rsquared(self):\n",
    "        return 1 - (self.sigma() ** 2) / self._mse_total()\n",
    "\n",
    "    def _coefficients_var(self):\n",
    "        return self.sigma() * np.sqrt(np.diag(self.comp))\n",
    "\n",
    "    def _coefficients_ic(self, significance=0.05):\n",
    "        tpvalue = st.t.ppf(1 - significance / 2, self._n[0] - self._n[1])\n",
    "        error = tpvalue * self._coefficients_var()\n",
    "        return self.coefficients() - error, self.coefficients() + error\n",
    "\n",
    "    def t_value(self):\n",
    "        return self.coefficients() / self._coefficients_var()\n",
    "\n",
    "    def t_pvalue(self):\n",
    "        return (1 - st.t.cdf(abs(self.t_value()), self.df_error())) * 2\n",
    "\n",
    "    def rstandard(self):\n",
    "        return self.residuals() / self.sigma()\n",
    "\n",
    "    def _hat(self):\n",
    "        return self.X @ self.comp @ self.X.T\n",
    "\n",
    "    def rstudent(self):\n",
    "        return self.residuals() / (self.sigma() * np.sqrt(1 - np.diag(self._hat())))\n",
    "\n",
    "    def _get_residuals(self, which=\"regression\"):\n",
    "        match which:\n",
    "            case \"regression\":\n",
    "                return self.residuals()\n",
    "            case \"studentized\":\n",
    "                return self.rstudent()\n",
    "            case \"standardized\":\n",
    "                return self.rstandard()\n",
    "\n",
    "    def _cook_distance(self):\n",
    "        return (self.rstudent() ** 2 * np.diag(self._hat())) / (\n",
    "            self._n[1] * (1 - np.diag(self._hat()))\n",
    "        )\n",
    "\n",
    "    def _some_sup_tests(\n",
    "        self, res=\"regression\", test_norm=\"shapiro\", test_het=\"breusch\"\n",
    "    ):\n",
    "        residuals = self._get_residuals(res)\n",
    "\n",
    "        match test_norm:\n",
    "            case \"lilliefors\":\n",
    "                normality = sm.stats.diagnostic.lilliefors(residuals)\n",
    "            case \"shapiro\":\n",
    "                normality = st.shapiro(residuals)\n",
    "            case \"anderson\":\n",
    "                normality = st.anderson(residuals)\n",
    "            case _:\n",
    "                raise ValueError(f\"{test_norm} isn't supported in scratch_models\")\n",
    "\n",
    "        match test_het:\n",
    "            case \"breusch\":\n",
    "                het = sm.stats.diagnostic.het_breuschpagan(self.residuals(), self.X)\n",
    "            case \"gold\":\n",
    "                het = sm.stats.diagnostic.het_goldfeldquandt(self.y, self.X)\n",
    "            case _:\n",
    "                raise ValueError(f\"{test_het} isn't supported in scratch_models\")\n",
    "\n",
    "        return {\"Normality\": normality[0:2], \"heteroscedasticity\": het[0:2]}\n",
    "\n",
    "    def summary(self):\n",
    "\n",
    "        some_stats_left = [\n",
    "            (\"Dep. Variable:\", self._ycol),\n",
    "            (\"Log-Likelihood:\", self._loglik()),\n",
    "            (\"F-statistic:\", self.f_value()),\n",
    "            (\"Df Model:\", self.df_model()),\n",
    "            (\"BIC:\", self._bic()),\n",
    "        ]\n",
    "\n",
    "        some_stats_right = [\n",
    "            (\"R-squared:\", self.rsquared()),\n",
    "            (\"Adj. R-squared:\", self.adj_rsquared()),\n",
    "            (\"Prob (F-statistic):\", self.f_pvalue()),\n",
    "            (\"Df Residuals:\", self.df_error()),\n",
    "            (\"AIC:\", self._aic()),\n",
    "        ]\n",
    "\n",
    "        some_star = \"*\" * 80\n",
    "\n",
    "        print(\"LinearRegression\".center(80))\n",
    "        print(some_star)\n",
    "\n",
    "        for i in range(len(some_stats_left)):\n",
    "            right_vals1 = some_stats_right[i][0]\n",
    "            right_vals2 = some_stats_right[i][1]\n",
    "            left_vals1 = some_stats_left[i][0]\n",
    "            left_vals2 = some_stats_left[i][1]\n",
    "            diff_left = 39 - len(left_vals1)\n",
    "            diff_right = 36 - len(right_vals1)\n",
    "            string_right = f\"  {right_vals1} {right_vals2:>{diff_right}.3f}\"\n",
    "            if isinstance(left_vals2, str):\n",
    "                print(f\"{left_vals1} {left_vals2:>{diff_left}}\", string_right)\n",
    "            else:\n",
    "                print(f\"{left_vals1} {left_vals2:>{diff_left}.3f}\", string_right)\n",
    "\n",
    "        print(some_star)\n",
    "        print(\n",
    "            \" \" * 20\n",
    "            + f\"{'coef':<9} {'std err':<10} {'t':<10} {'P>|t|':<10} {'[0.025':<10} {'0.975]':<10}\"\n",
    "        )\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "        for i in range(len(self._cols)):\n",
    "            coef, std_err = self.coefficients()[i], self._coefficients_var()[i]\n",
    "            t_val, p_val = self.t_value()[i], self.t_pvalue()[i]\n",
    "            interval_low, interval_high = (\n",
    "                self._coefficients_ic()[0][i],\n",
    "                self._coefficients_ic()[1][i],\n",
    "            )\n",
    "            p_val_formatted = (\n",
    "                \"< 2e-16\"\n",
    "                if p_val < 2e-16\n",
    "                else f\"{p_val:.3f}\" if p_val >= 1e-3 else f\"{p_val:.2e}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"{self._cols[i][0:16]:<18} {coef:<10.4f} {std_err:<10.3f} {t_val:<10.3f} {p_val_formatted:<10} {interval_low:<10.3f} {interval_high:<10.3f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "class LinearRegression(BaseMetrics):\n",
    "    def __init__(self, X, y, intercept=True):\n",
    "        super().__init__(X, y, intercept)\n",
    "\n",
    "    def fit(self):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.intercept:\n",
    "            preds = (\n",
    "                X.reindex([\"C\", *self._cols], axis=1).assign(C=1).to_numpy()\n",
    "                @ self.coefficients()\n",
    "            )\n",
    "            return preds\n",
    "        else:\n",
    "            preds = X.reindex([*self._cols], axis=1).to_numpy() @ self.coefficients\n",
    "            return preds\n",
    "\n",
    "    def vis_normal(self, **kwargs):\n",
    "        _, ax = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "        residuals = self._get_residuals(**kwargs)\n",
    "        x = np.linspace(np.min(residuals), np.max(residuals), 1000)\n",
    "        y = st.norm.pdf(x, np.mean(residuals), np.std(residuals))\n",
    "        sm.qqplot(residuals, line=\"q\", ax=ax[0])\n",
    "        sns.kdeplot(x=residuals, ax=ax[1])\n",
    "        sns.lineplot(x=x, y=y, ax=ax[1], color=\"red\")\n",
    "\n",
    "    def vis_linear(self, **kwargs):\n",
    "        sns.relplot(x=self.y, y=self.fitted(), **kwargs) \\\n",
    "        .set_axis_labels(\"Obs. values\", \"Fitted values\") \\\n",
    "        .ax.axline(xy1 = (0, 0), slope = 1, color=\"gray\", alpha=0.4, dashes=(2, 2))\n",
    "\n",
    "    def vis_homo(self, which=\"regression\", **kwargs):\n",
    "        residuals = self._get_residuals(which=which)\n",
    "        sns.relplot(x=self.fitted(), y=residuals, **kwargs) \\\n",
    "        .set_axis_labels(\"Fitted values\", \"Residuals\") \\\n",
    "        .ax.axline(xy1 = (0, 0), slope = 0, color=\"gray\", alpha=0.4, dashes=(2, 2))\n",
    "\n",
    "    def vis_arr(self, which=\"regression\", **kwargs):\n",
    "        residuals = self._get_residuals(which)\n",
    "        sns.relplot(x=range(self._n[0]), y=residuals, **kwargs) \\\n",
    "        .set_axis_labels(\"Index\", \"Residuals\") \\\n",
    "        .ax.axline(xy1 = (0, 0), slope = 0, color=\"gray\", alpha=0.4, dashes=(2, 2))\n",
    "\n",
    "    def vis_anomalies(self, significance=0.5, **kwargs):\n",
    "        fig, ax = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "        h = np.diag(self._hat())\n",
    "        residuals = self._get_residuals(which=\"studentized\")\n",
    "        sns.scatterplot(x=self.fitted(), y=h, ax=ax[0], **kwargs)\n",
    "        ax[0].axline(xy1 = (0, 2 * self._n[1] / self._n[0]),\n",
    "                   slope = 0, color=\"gray\", alpha=0.4, dashes=(2, 2))\n",
    "        ax[0].set_xlabel(\"Fitted values\")\n",
    "        ax[0].set_ylabel(\"hii\")\n",
    "        \n",
    "\n",
    "        li_cook = st.f.ppf(0.5, significance, self._n[1], self._n[0] - self._n[1])\n",
    "        sns.scatterplot(x=self.fitted(), y=self._cook_distance(), ax=ax[1], **kwargs)\n",
    "        ax[1].axline(xy1 = (0, li_cook), slope=0, color=\"gray\", dashes=(2, 2))\n",
    "        ax[1].set_xlabel(\"Fitted values\")\n",
    "        ax[1].set_ylabel(\"Di\")\n",
    "    \n",
    "    def diagnostics(self, X=None):\n",
    "        if X:\n",
    "            pass\n",
    "        else:\n",
    "            fig, ax = plt.subplots(2, 2, figsize=(12, 6))\n",
    "            ind = [*range(1, self._n[0] + 1)]\n",
    "            sns.scatterplot(\n",
    "                x=self.X @ self.coefficients(), y=self.residuals(), ax=ax[0, 0]\n",
    "            )\n",
    "            ax[0, 0].axhline(0, color=\"red\")\n",
    "            sm.qqplot(self.residuals(), line=\"q\", ax=ax[1, 0])\n",
    "            sns.scatterplot(x=ind, y=self._cook_distance(), ax=ax[0, 1])\n",
    "            sns.scatterplot(x=ind, y=np.diag(self._hat()), ax=ax[1, 1])\n",
    "\n",
    "\n",
    "X = data.drop(columns=\"Overall\")\n",
    "y = data.Overall\n",
    "\n",
    "reg = LinearRegression(X=X, y=y, intercept=True).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1bf1d2ca-76a7-494c-bd83-9c4c611cf335",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseMetrics.residuals() got an unexpected keyword argument 'which'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvis_homo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[113], line 249\u001b[0m, in \u001b[0;36mLinearRegression.vis_homo\u001b[0;34m(self, which, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvis_homo\u001b[39m(\u001b[38;5;28mself\u001b[39m, which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 249\u001b[0m     residuals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresiduals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhich\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhich\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     sns\u001b[38;5;241m.\u001b[39mrelplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted(), y\u001b[38;5;241m=\u001b[39mresiduals, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \\\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;241m.\u001b[39mset_axis_labels(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitted values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResiduals\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;241m.\u001b[39max\u001b[38;5;241m.\u001b[39maxline(xy1 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), slope \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, dashes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: BaseMetrics.residuals() got an unexpected keyword argument 'which'"
     ]
    }
   ],
   "source": [
    "reg.vis_homo(color = \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bdff2bb-5877-4982-9506-4eb1a2893aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spector_data = sm.datasets.spector.load()\n",
    "spector_data.exog = sm.add_constant(spector_data.exog, prepend=False)\n",
    "mod = sm.OLS(spector_data.endog, spector_data.exog)\n",
    "res = mod.fit()\n",
    "batata = res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ed630f1-1860-4cce-97e3-e349e87fed56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Omnibus:</td>\n",
       "      <td>0.176</td>\n",
       "      <td>Durbin-Watson:</td>\n",
       "      <td>2.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prob(Omnibus):</td>\n",
       "      <td>0.916</td>\n",
       "      <td>Jarque-Bera (JB):</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skew:</td>\n",
       "      <td>0.141</td>\n",
       "      <td>Prob(JB):</td>\n",
       "      <td>0.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kurtosis:</td>\n",
       "      <td>2.786</td>\n",
       "      <td>Cond. No.</td>\n",
       "      <td>176.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0       1                      2         3\n",
       "0        Omnibus:   0.176    Durbin-Watson:          2.346\n",
       "1  Prob(Omnibus):   0.916    Jarque-Bera (JB):       0.167\n",
       "2           Skew:   0.141    Prob(JB):               0.920\n",
       "3       Kurtosis:   2.786    Cond. No.                176."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(batata.tables[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9cc0e9-7af7-4312-9a84-5311356b47cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
